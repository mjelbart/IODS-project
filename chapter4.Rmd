# Chapter 4: Clustering and classification
## 2. The dataset
### A brief description of the dataset including structure and dimensions.
The dataset regards Housing Values in Suburbs of Boston including 14 variables such as median value of owner-occupied homes (medv, in \$1000s) crime rate per capita by town (crim) and average number of rooms per dwelling (rm). There are 506 observations. 

```{r setup, include=FALSE}
library(corrplot)
library(tidyverse)
library(MASS)

Boston <- read.table("data/boston3.csv", header=TRUE, sep=",", fileEncoding = 'UTF-8-BOM')

```

```{r}
dim(Boston)
str(Boston)
```

Variable descriptions:

crim: per capita crime rate by town.

zn: proportion of residential land zoned for lots over 25,000 sq.ft.

indus: proportion of non-retail business acres per town. 

chas: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).

nox: nitrogen oxides concentration (parts per 10 million).

rm: average number of rooms per dwelling.

age: proportion of owner-occupied units built prior to 1940.

dis: weighted mean of distances to five Boston employment centres.

rad: index of accessibility to radial highways.

tax: full-value property-tax rate per $10,000.

ptratio: pupil-teacher ratio by town.

black: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.

lstat: lower status of the population (percent).

medv: median value of owner-occupied homes in $1000s.


## 3. Overview 
### A graphical overview of the data and interpretations
In the correlation matrix plot below we can see a few relationships. 

Strong positive correlations include rad-tax, indus-nox, nox-age, rm-medv, crim-rad, zn-dis, indus-tax. 

Strong negative correlations include age-dis, lstat-medv, nox-dis, indus-dis

```{r}
library(corrplot)
library(tidyverse)

# generate the correlation matrix
cor_matrix<-cor(Boston) %>% round(digits = 2)

# plot the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos = "d", tl.cex = 0.6)
```

### A summary of the variables in the data.
In the summary information below we can see that black seems to have an outlier with the Min. value being far below that of any of the quantiles.

```{r}
summary(Boston)
```



## 4. Standardizing the dataset
### Standardization of the dataset and summaries of the scaled data.
Below the dataset is standardized by scaling. In the summary of the of the scaled data we can see a change - the variables now all have a mean of zero.
```{r}
# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

```

### Creating a categorical variable of the crime rate in the Boston dataset from the scaled crime rate and using the quantiles as the break points in the categorical variable.
```{r}
# summary of the scaled crime rate
summary(boston_scaled$crim)

# create a quantile vector of crim
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
labelvector <- c("low", "med_low", "med_high", "high")
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label=labelvector)
```

### Dropping the old crime rate variable from the dataset. 

```{r}
# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

```

### Dividing the dataset to train and test sets, so that 80% of the data belongs to the train set.
Below the dataset is divided into train and test sets with 80% of the data in the train set.
```{r}
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set by selecting the row numbers that are saved in ind.
train <- boston_scaled[ind,]

# create test set by subtracting the rows that are used in the train set (note the minus sign)
test <- boston_scaled[-ind,]

```


## 5. Linear Discriminant Analysis (LDA)
### Fitting and plotting the LDA
Below we fit the LDA on the train set using the categorical crime rate as the target variable and all the other variables in the dataset as predictor variables, then plot the result.

```{r}
lda.fit <- lda(crime ~ ., data = train)

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)

```

## 6. Predicting
Below we save the crime categories from the test set and then remove the categorical crime variable from the test dataset.
```{r}
# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)
```

Below we predict the classes with the LDA model on the test data and cross tabulate the results with the crime categories from the test set. The predictions are mostly quite good with most of the predicted values lining up with the correct values. It was a pit poor at the med_low values with 10 low values and 7 med_high values included in the med_low predicted class.

```{r}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results.
table(correct = correct_classes, predicted = lda.pred$class)
```

## 7. Clustering
Reload the Boston dataset and standardize the dataset

```{r}
# load MASS and Boston
library(MASS)
data('Boston')
boston_scaled <- scale(Boston)
# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
```

### Calculate the distances between the observations. 
```{r}
# euclidean distance matrix (euclidean is the default method of dist())
dist_eu <- dist(boston_scaled)

# look at the summary of the distances
summary(dist_eu)
```

### Running the k-means algorithm on the dataset and plotting pairs
```{r}
km <-kmeans(Boston, centers = 3)
pairs(Boston, col = km$cluster)
```

### Determining the optimal number of clusters
The radical drop around 2 in the plot below shows us that the optimal number of clusters is 2.
```{r}
set.seed(123)
 
# determine the number of clusters
k_max <- 10
 
# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})
 
# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
 
```


### Running the k-means algorithm again with the optimal number of clusters
```{r}
km <-kmeans(Boston, centers = 2)
```


### Visualization of the clusters 
The clusters are marked with different colors, by having just two centroids we can see that for many variables the clusters are not too intermixed. The clustering indicates that there are potential correlations worth exploring in the dataset that might not have been apparent from the correlation plot performed at the start.

```{r}
pairs(Boston, col = km$cluster)
```


